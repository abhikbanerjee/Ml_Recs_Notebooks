{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is based on the blogpost for understanding NLP blog post \n",
    "\n",
    "https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "pd.set_option(\"display.max.rows\", 99)\n",
    "pd.set_option(\"display.max.columns\", 99)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a method to get the news arcticles and parse them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_url = ['https://inshorts.com/en/read/technology',\n",
    "           'https://inshorts.com/en/read/sports',\n",
    "           'https://inshorts.com/en/read/world']\n",
    "\n",
    "def build_dataset(seed_url):\n",
    "    news_data = []\n",
    "    for link in seed_url:\n",
    "        news_category = link.split(\"/\")[-1]\n",
    "#         print(news_category)\n",
    "        data = requests.get(link)\n",
    "        cleaned_data = BeautifulSoup(data.content, 'html.parser')\n",
    "    \n",
    "        # Parse the HTML to create structured content\n",
    "        news_articles = [{\n",
    "            'news_headline': headline.find('span', attrs={\"itemprop\": \"headline\"}).string, 'news_article': article.find('div', attrs={\"itemprop\": \"articleBody\"}).string,'news_category': news_category}                         \n",
    "            for headline, article in zip(cleaned_data.find_all('div', class_=[\"news-card-title\"]), cleaned_data.find_all('div', class_=[\"news-card-content\"]))                        ]\n",
    "        news_data.extend(news_articles)\n",
    "        \n",
    "    \n",
    "    df = pd.DataFrame(news_data, columns=['news_headline', 'news_article', 'news_category'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the build_dataset to create the dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_headline</th>\n",
       "      <th>news_article</th>\n",
       "      <th>news_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What's the story behind egg pic that broke Kyl...</td>\n",
       "      <td>Instagram account that broke Kylie Jenner's re...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Facebook Messenger now lets users unsend mess...</td>\n",
       "      <td>Facebook Messenger has globally rolled out \"Re...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Even my kids said it was cringey: YouTube CEO ...</td>\n",
       "      <td>YouTube CEO Susan Wojcicki has revealed even h...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I hated being only woman in the room: Randi Zu...</td>\n",
       "      <td>Facebook Co-founder Mark Zuckerberg's elder si...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm going to shoot up a school: US boy tells S...</td>\n",
       "      <td>A middle school student in Indiana, US got a l...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Supermodel Tyra Banks to open tech-based attra...</td>\n",
       "      <td>American supermodel-turned-entrepreneur Tyra B...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>WhatsApp to ban political parties if misused a...</td>\n",
       "      <td>WhatsApp's Communications head Carl Woog warne...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Facebook agrees to share user chat details wit...</td>\n",
       "      <td>Facebook has agreed to share 'Messenger' chat ...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Flickr deletes photos of free accounts going o...</td>\n",
       "      <td>Photo and video-hosting platform Flickr on Tue...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Reddit to raise up to $300mn at about $3bn val...</td>\n",
       "      <td>Online discussions platform Reddit may raise $...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       news_headline  \\\n",
       "0  What's the story behind egg pic that broke Kyl...   \n",
       "1   Facebook Messenger now lets users unsend mess...   \n",
       "2  Even my kids said it was cringey: YouTube CEO ...   \n",
       "3  I hated being only woman in the room: Randi Zu...   \n",
       "4  I'm going to shoot up a school: US boy tells S...   \n",
       "5  Supermodel Tyra Banks to open tech-based attra...   \n",
       "6  WhatsApp to ban political parties if misused a...   \n",
       "7  Facebook agrees to share user chat details wit...   \n",
       "8  Flickr deletes photos of free accounts going o...   \n",
       "9  Reddit to raise up to $300mn at about $3bn val...   \n",
       "\n",
       "                                        news_article news_category  \n",
       "0  Instagram account that broke Kylie Jenner's re...    technology  \n",
       "1  Facebook Messenger has globally rolled out \"Re...    technology  \n",
       "2  YouTube CEO Susan Wojcicki has revealed even h...    technology  \n",
       "3  Facebook Co-founder Mark Zuckerberg's elder si...    technology  \n",
       "4  A middle school student in Indiana, US got a l...    technology  \n",
       "5  American supermodel-turned-entrepreneur Tyra B...    technology  \n",
       "6  WhatsApp's Communications head Carl Woog warne...    technology  \n",
       "7  Facebook has agreed to share 'Messenger' chat ...    technology  \n",
       "8  Photo and video-hosting platform Flickr on Tue...    technology  \n",
       "9  Online discussions platform Reddit may raise $...    technology  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = build_dataset(seed_url)\n",
    "news_df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sports        25\n",
       "world         25\n",
       "technology    24\n",
       "Name: news_category, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# news_df.news_category.value_counts(normalize=True)\n",
    "news_df.news_category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the necessary nltk tokenizers and unicodedata and contraction maps\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import unicodedata\n",
    "from contractions import CONTRACTION_MAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwods from the nltk english corpus\n",
    "\n",
    "nlp = spacy.load('en', parse = True, tag=True, entity=True)\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.remove('no')\n",
    "stopwords.remove('not')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The text pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(data):\n",
    "    text = BeautifulSoup(data, \"html.parser\")\n",
    "    return text.get_text()\n",
    "\n",
    "def remove_accented_characters(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "# This method is for expanding contractions, read up on how the regex and match works\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    \n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "def remove_special_chars(text, delete_num=False):\n",
    "    compile_pattern = r'[^A-Za-z0-9\\s]' if not delete_num else r'[^A-Za-z\\s]'\n",
    "    text = re.sub(compile_pattern, \"\",text)\n",
    "    return text\n",
    "\n",
    "def simple_stemmer(text):\n",
    "    nps = nltk.stem.PorterStemmer()\n",
    "    return \" \".join([nps.stem(word) for word in text.split()])\n",
    "\n",
    "def simple_lemmatization(text):\n",
    "    words = nlp(text)\n",
    "    lemma_words = [word.lemma_ if word.lemma_!=\"-PRON-\" else word.text for word in words]\n",
    "    return \" \".join([word for word in lemma_words])\n",
    "\n",
    "def remove_stopwords(text, to_lower=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    final_words = []\n",
    "    if to_lower:\n",
    "        final_words = [token for token in tokens if token not in stopwords ]\n",
    "    else:\n",
    "        final_words = [token for token in tokens if token.lower() not in stopwords ]\n",
    "        \n",
    "    final_words_ns = \" \".join(final_words)\n",
    "    return final_words_ns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now combine all the methods into the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(corpus, html_cleaner=True, remove_acccented_chars = True, contractions_expansion=True,\n",
    "                      rmv_spl_chars = True, to_lower_case=True, smpl_stemmer=True, smpl_lemmatization=True,\n",
    "                      rmv_stopwords=True):\n",
    "    normalized_corpus = []\n",
    "    for doc in corpus:\n",
    "#         print(doc)\n",
    "        # clean html tags\n",
    "        if html_cleaner:\n",
    "            doc = clean_html(doc)\n",
    "        # remove accented characters\n",
    "        if remove_acccented_chars:\n",
    "            doc = remove_accented_characters(doc)\n",
    "        # expand the contractions\n",
    "        if contractions_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "        # remove special characters\n",
    "        if rmv_spl_chars:\n",
    "            doc = remove_special_chars(doc)\n",
    "        # converting to lower case\n",
    "        if to_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # apply simple stemming\n",
    "        if smpl_stemmer:\n",
    "            doc = simple_stemmer(doc)\n",
    "        # apply lemmatization\n",
    "        if smpl_lemmatization:\n",
    "            doc = simple_lemmatization(doc)\n",
    "        # apply stopwords removal\n",
    "        if rmv_stopwords:\n",
    "            doc = remove_stopwords(doc)\n",
    "        normalized_corpus.append(doc)        \n",
    "    return normalized_corpus\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the preprocessing function on the dataframe\n",
    "news_df['full_text'] = news_df['news_headline']+\". \"+news_df['news_article']\n",
    "news_df['clean_text'] = text_preprocessing(news_df['full_text'])\n",
    "# check a sample as dictionary for clean and full text\n",
    "news_df[[\"full_text\", \"clean_text\"]].iloc[7].to_dict()\n",
    "\n",
    "# save the news_df to the csv file\n",
    "news_df.to_csv('news.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Language and Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS tag</th>\n",
       "      <th>Tag type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>agrees</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>share</td>\n",
       "      <td>VB</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user</td>\n",
       "      <td>NN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>chat</td>\n",
       "      <td>NN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>details</td>\n",
       "      <td>NNS</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>with</td>\n",
       "      <td>IN</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Delhi</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Police</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word POS tag Tag type\n",
       "0  Facebook     NNP    PROPN\n",
       "1    agrees     VBZ     VERB\n",
       "2        to      TO     PART\n",
       "3     share      VB     VERB\n",
       "4      user      NN     NOUN\n",
       "5      chat      NN     NOUN\n",
       "6   details     NNS     NOUN\n",
       "7      with      IN      ADP\n",
       "8     Delhi     NNP    PROPN\n",
       "9    Police     NNP    PROPN"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = str(news_df.iloc[7].news_headline)\n",
    "\n",
    "# Run POS Tagging using spacy\n",
    "# %timeit sentence_nlp = nlp(sentence)  # For timing the pos tag operation using spacy\n",
    "sentence_nlp = nlp(sentence)\n",
    "spacy_pos_tagged = [(word, word.tag_, word.pos_) for word in sentence_nlp]\n",
    "pd.DataFrame(spacy_pos_tagged, columns=['Word', 'POS tag', 'Tag type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>agrees</td>\n",
       "      <td>VBZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>share</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>chat</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>details</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>with</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Delhi</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Police</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word POS tag\n",
       "0  Facebook     NNP\n",
       "1    agrees     VBZ\n",
       "2        to      TO\n",
       "3     share      NN\n",
       "4      user      JJ\n",
       "5      chat      NN\n",
       "6   details     NNS\n",
       "7      with      IN\n",
       "8     Delhi     NNP\n",
       "9    Police     NNP"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the pos tagging using NLTK\n",
    "# %timeit nltk_pos_tagged = nltk.pos_tag(sentence.split()) # For timing the pos tag operation using spacy\n",
    "nltk_pos_tagged = nltk.pos_tag(sentence.split())\n",
    "pd.DataFrame(nltk_pos_tagged, columns=['Word', 'POS tag'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(The, 'DT', 'DET'),\n",
       " (quick, 'JJ', 'ADJ'),\n",
       " (brown, 'JJ', 'ADJ'),\n",
       " (fox, 'NN', 'NOUN'),\n",
       " (jumped, 'VBD', 'VERB'),\n",
       " (over, 'IN', 'ADP'),\n",
       " (the, 'DT', 'DET'),\n",
       " (lazy, 'JJ', 'ADJ'),\n",
       " (dog, 'NN', 'NOUN')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"The quick brown fox jumped over the lazy dog\"\n",
    "sent_nlp = nlp(sent)\n",
    "spacy_pos_tagged = [(word, word.tag_, word.pos_) for word in sent_nlp]\n",
    "spacy_pos_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abhik', 'is', 'testing', 'something']\n",
      "[('abhik', 'is'), ('is', 'testing'), ('testing', 'something')]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.util import ngrams\n",
    "\n",
    "s = \"Abhik is testing something\"\n",
    "s = s.lower()\n",
    "tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "print(tokens)\n",
    "output = list(ngrams(tokens, 2))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abhik is', 'is testing', 'testing something']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt = [\"\".join([word[0],\" \",word[1]]) for word in output]\n",
    "pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the functions to test each functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the clean_html function\n",
    "cleaned_html = clean_html(\"<html><head>Hello how are you. </head><p>I am here to check the text</p></html>\")\n",
    "\n",
    "# check the remove accented characters function\n",
    "cleaned_accented = remove_accented_characters('Sómě Áccěntěd těxt')\n",
    "print(cleaned_accented)\n",
    "\n",
    "# check the expand contractions\n",
    "print(expand_contractions(\"Y'all can't expand contractions I'd think\"))\n",
    "\n",
    "\n",
    "# remove the numbers from text\n",
    "print(remove_special_chars(\"Hello! How are you ? #865875\"))\n",
    "# print(remove_special_chars(\"Hello! How are you ? #865875\", True))\n",
    "\n",
    "# call the porter stemmer\n",
    "text = \"Hello! How are you running jumping sleeping\"\n",
    "print(simple_stemmer(text))\n",
    "\n",
    "# call the simple Lemmatizer\n",
    "print(simple_lemmatization(text))\n",
    "\n",
    "# check the remove_stopwords function\n",
    "print(remove_stopwords(\"I am going to go to market to eat some lunch\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
